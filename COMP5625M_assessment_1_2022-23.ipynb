{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5625M Assessment 1 - Image Classification [100 marks]\n",
    "\n",
    "<div class=\"logos\"><img src=\"https://drive.google.com/uc?id=132BXgkV5w1bpXlVpdr5BtZdpagqYvna7\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this assessment, you will gain practical experience in:\n",
    "\n",
    "> 1. Implementing and evaluating a multi-layer perceptron (MLP) and convolutional neural network (CNN) in solving a classification problem\n",
    "> 2. Building, evaluating, and finetuning a CNN on an image dataset from development to testing \n",
    "> 3. Tackling overfitting using strategies such as data augmentation and drop out\n",
    "> 4. Fine tuning a model \n",
    "> 5. Comparing the performance of a new model with an off-the-shelf model (AlexNet)\n",
    "> 6. Gaining a deeper understanding of model performance using visualisations from Grad-CAM.\n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU. We highly recommend you use platforms such as Colab.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from a direct link or the Kaggle challenge website:\n",
    "\n",
    ">[Direct access to data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[Access data through Kaggle webpage](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07) \n",
    "\n",
    "\n",
    "### Required submissions\n",
    "\n",
    "##### 1. Kaggle Competition\n",
    "To participate in the submission of test results, you will need an account. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n",
    "\n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. [Link to submit your results on Kaggle competition](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/submissions). \n",
    "\n",
    "Please submit only your predictions from test set - detailed instructions are provided in (3)\n",
    "\n",
    "##### 2. Submission of your work\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected images from section 6 \"Failure/success analysis\" (outputs from gradcam, for example you can put these images into failure and succcess folders).\n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is a package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is a package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is a library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed - examples of importing libraries are provided below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview:**\n",
    "\n",
    "**1. Function implementation** (12 marks)\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for a simple MLP model (4 marks)\n",
    "*   **1.3** PyTorch ```Model``` class for a simple CNN model (4 marks)\n",
    "\n",
    "**2. Model training** (20 marks)\n",
    "*   **2.1** Train on TinyImageNet30 dataset (7 marks)\n",
    "*   **2.2** Generate confusion matrices and ROC curves (4 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (9 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "            \n",
    "\n",
    "**3. Model Fine-tuning on CIFAR10 dataset** (20 marks)\n",
    "*   **3.1** Fine-tune your model (initialise your model with pretrained weights from (2)) (8 marks)\n",
    "*   **3.2** Fine-tune model with frozen base convolution layers (8 marks)\n",
    "*   **3.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe? (4 marks) \n",
    "\n",
    "**4. Model testing** (18 marks)\n",
    "*   **4.1**   Test your final model in (2) on test set - code to do this (10 marks)\n",
    "*   **4.2**   Upload your result to Kaggle  (8 marks)\n",
    "\n",
    "**5. Model comparison** (14 marks)\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (6 marks)\n",
    "*   **5.2**   Compare the results of your CNN model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks)\n",
    "\n",
    "**6. Interpretation of results** (16 marks)\n",
    "*   **6.1** Use grad-CAM on your model and on AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare the results from your model and from AlexNet (4 marks)\n",
    "*   **6.3** Comment on (6 marks):\n",
    "    - why the network predictions were correct or not correct in your predictions? \n",
    "    - what can you do to improve your results further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Function implementations [12 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset class [4 marks]\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a MLP model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Input Units\n",
    "- Hidden Units\n",
    "- Output Units\n",
    "- Activation functions\n",
    "- Loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a MLP Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define a CNN model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers \n",
    "- Loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model training [20 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train both MLP and CNN models - show loss and accuracy graphs side by side [7 marks]\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. Top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top-*k* accuracy \n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running you MLP model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running you CNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on your model and results that should include number of parameters in each model and why CNN over MLP for image classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves [4 marks]\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way that clearly indicates what percentage of the data is represented in each position.\n",
    "- Display ROC curve for 5 top classes with area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: All parts below here relate to the CNN model only and not the MLP! You are advised to use your final CNN model only for each of the following parts.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Strategies for tackling overfitting (9 marks)\n",
    "Using your (final) CNN model, use the strategies below to avoid overfitting. You can reuse the network weights from previous training, often referred to as ``fine tuning``. \n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Data augmentation\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations. \n",
    "\n",
    "> Provide graph and comment on what you observe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Dropout\n",
    "\n",
    "> Implement dropout in your model \n",
    "\n",
    "> Provide graph and comment on your choice of proportion used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Hyperparameter tuning\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001]\n",
    "\n",
    "> Provide separate graphs for loss and accuracy, each showing performance at three different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model testing [18 marks]\n",
    "Online evaluation of your model performance on the test set.\n",
    "\n",
    "> Prepare the dataloader for test set\n",
    "\n",
    "> Write evaluation code for writing predictions\n",
    "\n",
    "> Upload it to Kaggle submission page [link](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Test class and predictions [10 marks]\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions \n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle [8 marks]\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "Please note you will get marks for higher performance.\n",
    "\n",
    "The ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image in the test set and 1 row for the headers. [To submit please visit](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [8 marks]. The class leaderboard will not affect marking (brownie points!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [20 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one task and then tunes or tweaks the model to make it perform a second similar task. You can perform finetuning in the following way:\n",
    "- Train an entire model from scratch (large dataset, more computation)\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation) \n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10``, [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here are only 10*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Apply transfer learning initialise with pretrained model weights\n",
    "Use pretrained weights from AlexNet only (on the right of figure) to initialise your model. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg\" style=\"width:1000px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: Two models are given here: LeNet and AlexNet for image classification. However, you have to use **only AlexNet**.</center></caption>\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compare above configurations and comment on comparative performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Model comparisons\n",
    "We often need to compare our model with other state-of-the-art methods to understand how well it performs compared to existing architectures. Here you will thus compare your model design with AlexNet on the TinyImageNet30 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Finetune AlexNet on TinyImageNet30\n",
    "> Load AlexNet as you did above\n",
    "\n",
    "> Train AlexNet on TinyImageNet30 dataset until convergence. Make sure you use the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Compare results on validation set of TinyImageNet30\n",
    "> Loss graph, top1 accuracy, confusion matrix and execution time for your model (say, mymodel and AlexNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Interpretation of results (16 marks)\n",
    "\n",
    "> Please use TinyImageNet30 dataset for all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 6.1-6.2 Implement grad-CAM and visualise results (10 marks)\n",
    "\n",
    "- Use an existing library to initiate grad-CAM \n",
    "\n",
    "        - To install: !pip install torchcam\n",
    "        - Call SmoothGradCAMpp: from torchcam.methods import SmoothGradCAMpp\n",
    "        - Apply to your model \n",
    "\n",
    "You can see the details here: https://github.com/frgfm/torch-cam\n",
    "\n",
    "- Apply grad-CAM to your model on at least four correctly classified images\n",
    "- Apply grad-CAM on retrained AlexNet on at least four incorrectly classified images\n",
    "\n",
    ">It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n",
    "\n",
    "\n",
    "**HINT for displaying images with grad-CAM:**\n",
    "\n",
    "Display ```heatmap``` as a coloured heatmap superimposed onto the original image. We recommend the following steps to get a clear meaningful display. \n",
    "\n",
    "From torchcam.utils import overlay_mask. But remember to resize your image, normalise it and put a 1 for the batch dimension (e.g, [1, 3, 224, 224]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Your comments on (6 marks):\n",
    "> a) Why model predictions were correct or incorrect? You can support your case from 6.2\n",
    "\n",
    "> b) What can you do to improve your results further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> Double click to respond here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission. Use our teams channel to seek any help!**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
